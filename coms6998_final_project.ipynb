{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "This workbook does the following:\n",
        "\n",
        "\n",
        "1.   Import Raw Data\n",
        "2.   Create a all_func_grouped DF\n",
        "3.   Creates a dictionary of DFs for all the sub functions\n",
        "\n"
      ],
      "metadata": {
        "id": "BsdfrqUqXttz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only for Google Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Only run if you don't have the converted data yet\n",
        "# with open( '/content/drive/MyDrive/AzureFunctionsInvocationTraceForTwoWeeksJan2021.txt', 'r') as source:\n",
        "#   with open( '/content/drive/MyDrive/AzureFunctionsInvocationTraceForTwoWeeksJan2021.csv',\n",
        "# 'w') as target:\n",
        "#     target.write(source. read())\n"
      ],
      "metadata": {
        "id": "1n3EsFSDgEAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import datetime\n",
        "plt.rcParams['figure.figsize'] = (50, 20)  # Set figure size"
      ],
      "metadata": {
        "id": "NOCDQfaZLI6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "Now we have to download the data, build a combined dataset,\n",
        "a func-specific dataset, and then split each one of these into training, validation and test subsets."
      ],
      "metadata": {
        "id": "AvCFiBJp-q_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the raw data, then create a time value based on the end_timestamp\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/small_data_no_head.csv\")\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/AzureFunctionsInvocationTraceForTwoWeeksJan2021.csv\")\n",
        "\n",
        "df['time'] = [datetime.datetime(2021,1,31) + datetime.timedelta(seconds=x) for x in df['end_timestamp']]\n",
        "\n",
        "# Dataframe summarized\n",
        "print(f\"Number of Unique Applications: {len(set(df['func']))}\")\n",
        "df.info()\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "BaILyTPVLU9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Raw Data\n",
        "\n",
        "plt.plot(df['time'], df['duration'], 'o-')\n",
        "ax = plt.gca()\n",
        "ax.set_xlim([min(df['time']), max(df['time'])])\n",
        "ax.set_ylim([min(df['duration']), max(df['duration'])])\n",
        "plt.title('Duration Over Time (ALL APPLICATIONS)',  fontsize=40)\n",
        "plt.xlabel('End Timestamp', fontsize=20)\n",
        "plt.ylabel('Function Runtime Duration', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "szWmpJvLOt19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the all_func_grouped\n",
        "Now we need to convert this format to a time series usable format"
      ],
      "metadata": {
        "id": "wN95htK_SMId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to bin all the data\n",
        "\n",
        "# Bin the timestamps into 10-minute intervals\n",
        "df['time_bin'] = df['time'].dt.floor('10T')  # Floor the timestamps to the nearest 10 minutes\n",
        "\n",
        "# Group by these intervals and count the number of instances\n",
        "all_func_grouped = df.groupby('time_bin').size().reset_index(name='count')\n",
        "\n",
        "# Set the intervals as the index (optional)\n",
        "all_func_grouped.set_index('time_bin', inplace=True)\n",
        "\n",
        "all_func_grouped.info()"
      ],
      "metadata": {
        "id": "NQJNOx4JSPAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Distribution of the number of instances per bin\n",
        "all_func_grouped['count'].plot(kind='hist', bins=20, title='count')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SZhRMJ-YTM_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Number of instances over time, this is what we really want to predict\n",
        "all_func_grouped['count'].plot(kind='line', figsize=(8, 4), title='count')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MTxGjjsPTK40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duration Dataset Conversion\n",
        "Here we need to convert the dataset into a time series format\n",
        "with the duration as the only feature space."
      ],
      "metadata": {
        "id": "q0ParIyS2LDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 'time' as the index\n",
        "\n",
        "duration_df = df[['duration']]\n",
        "\n",
        "# # Resample to equally spaced time bins (e.g., 10 minutes)\n",
        "resampled_df = duration_df.resample('10T').mean()  # '10T' = 10-minute bins\n",
        "\n",
        "\n",
        "# # Handle missing bins (optional)\n",
        "resampled_df['duration'] = resampled_df['duration'].fillna(0)  # Fill with 0\n",
        "resampled_df.info()\n",
        "resampled_df"
      ],
      "metadata": {
        "id": "93oUWgq-2Ukx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "resampled_df['duration'].plot(kind='hist', bins=20, title='duration')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_YgQdHhu5ttT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "resampled_df['duration'].plot(kind='line', figsize=(8, 4), title='duration')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Mi8mvVSL5G8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the duration dataframe to an output csv:\n",
        "resampled_df.to_csv(\"/content/drive/MyDrive/AzureFunctionsInvocationTraceForTwoWeeksJan2021_resampled.csv\", index=True)"
      ],
      "metadata": {
        "id": "ANNO7Dr56fxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing func seperated data\n",
        "Now we are going to prepare a dictionary of dataframes to use\n",
        "where the keys are the function ids and the values are the\n",
        "grouped dataframes."
      ],
      "metadata": {
        "id": "bR5OPYfDYqDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the empty dictionary\n",
        "func_to_df = {}\n",
        "\n",
        "# Create the total time range\n",
        "start_time = df['time'].min().floor('10T')\n",
        "end_time = df['time'].max().ceil('10T')\n",
        "all_time_bins = pd.date_range(start=start_time, end=end_time, freq='10T')\n",
        "\n",
        "# Group the raw data by function id\n",
        "func_groups = df.groupby('func')\n",
        "\n",
        "# Iterate through each group and apply the binning and grouping logic\n",
        "for func_id, func_data in func_groups:\n",
        "    # Bin the timestamps into 10-minute intervals\n",
        "    func_data['time_bin'] = func_data['time'].dt.floor('10T')\n",
        "\n",
        "    # Group by the binned intervals and count the number of rows\n",
        "    grouped_func_data = func_data.groupby('time_bin').size().reset_index(name='count')\n",
        "\n",
        "    # Set the binned intervals as the index (optional)\n",
        "    grouped_func_data.set_index('time_bin', inplace=True)\n",
        "\n",
        "    # Reindex to include all time bins, filling missing bins with 0\n",
        "    grouped_func_data = grouped_func_data.reindex(all_time_bins, fill_value=0)\n",
        "\n",
        "    # Store the grouped DataFrame in the dictionary\n",
        "    func_to_df[func_id] = grouped_func_data"
      ],
      "metadata": {
        "id": "dzwxl94gYss_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dictionary and verify that the func specific data is correct\n",
        "\n",
        "for func_id, func_data in func_to_df.items():\n",
        "  print(\"func_id: \", func_id, len(func_data))\n",
        "\n",
        "  # Adds all of them on the plot\n",
        "  func_data['count'].plot(kind='line', figsize=(8, 4), title='count')\n",
        "  plt.gca().spines[['top', 'right']].set_visible(False)\n"
      ],
      "metadata": {
        "id": "DaRCu9K9Z9yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Lookback Datasets\n",
        "We need to create a lookback dataset, here we have the lookback as 10 for now."
      ],
      "metadata": {
        "id": "Ar3LY9_VApwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(data, lookback = 10):\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(lookback, len(data)):\n",
        "        X.append(data[i-lookback:i])\n",
        "        y.append(data[i])\n",
        "\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lookback = 20"
      ],
      "metadata": {
        "id": "WU3ivTPTFIoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### all_func_grouped Normalized & Lookback\n",
        "\n",
        "Here we are going to create a lookback dataset for all the functions added together."
      ],
      "metadata": {
        "id": "8bzUnkzYA1op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we need to normalize the data\n",
        "all_func_grouped['log1p_count'] = np.log1p(all_func_grouped['count'])\n",
        "\n",
        "# Now we create the all_func_grouped_X and all_func_grouped_y\n",
        "all_func_grouped_X, all_func_grouped_y = create_dataset(all_func_grouped['log1p_count'], lookback)\n",
        "\n",
        "# Display relevant information\n",
        "print(\"all_func_grouped_X\", np.shape(all_func_grouped_X))\n",
        "print(\"all_func_grouped_y\", np.shape(all_func_grouped_y))\n",
        "all_func_grouped.head()\n"
      ],
      "metadata": {
        "id": "4BCPxwv0BEFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### func seperated data normalized & lookback\n",
        "\n",
        "Here we are going to create a lookback dataset for all the functions added together."
      ],
      "metadata": {
        "id": "waonTIm5G9zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func_to_df_X = {}\n",
        "func_to_df_y = {}\n",
        "\n",
        "for func_id, func_data in func_to_df.items():\n",
        "  # First we need to normalize the data\n",
        "  func_data['log1p_count'] = np.log1p(func_data['count'])\n",
        "  func_to_df[func_id] = func_data  # Update df with log norm\n",
        "\n",
        "  # Now we create the func_data_X and func_data_y\n",
        "  func_data_X, func_data_y = create_dataset(func_data['log1p_count'], lookback)\n",
        "\n",
        "  # Populate the X and Y dictionaries\n",
        "  func_to_df_X[func_id] = func_data_X\n",
        "  func_to_df_y[func_id] = func_data_y\n",
        "\n",
        "  # Display relevant information\n",
        "  print(\"func_data_X\", np.shape(func_data_X))\n",
        "  print(\"func_data_y\", np.shape(func_data_y))\n",
        "  all_func_grouped.head()\n",
        "\n",
        "print(\"func_to_df_X: \", len(func_to_df_X))\n",
        "print(\"func_to_df_y: \", len(func_to_df_y))"
      ],
      "metadata": {
        "id": "sLin84TTHLGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Train, Valid, & Test subset\n",
        "\n",
        "We are going to follow this pattern:\n",
        "\n",
        "\n",
        "\n",
        "*   First 60% is Train\n",
        "*   Next 20% is Validation\n",
        "*   Last 20% is Test\n",
        "\n"
      ],
      "metadata": {
        "id": "4JZNkD3-_7jt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### all_func_grouped train, val, test"
      ],
      "metadata": {
        "id": "slY7Y9L7Adah"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "btr9OqMjb4Q7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}